## Aim
Aim of this excercise is to follow methodical approach to finetune the network.

##Training log
Train on 60000 samples, validate on 10000 samples
Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.0032.
60000/60000 [==============================] - 9s 153us/step - loss: 0.5335 - acc: 0.8471 - val_loss: 0.0869 - val_acc: 0.9841

Epoch 00001: val_acc improved from -inf to 0.98410, saving model to weights.best.hdf5
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0024260804.
60000/60000 [==============================] - 7s 118us/step - loss: 0.2503 - acc: 0.9250 - val_loss: 0.0612 - val_acc: 0.9873

Epoch 00002: val_acc improved from 0.98410 to 0.98730, saving model to weights.best.hdf5
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.001953602.
60000/60000 [==============================] - 7s 116us/step - loss: 0.1979 - acc: 0.9395 - val_loss: 0.0420 - val_acc: 0.9910

Epoch 00003: val_acc improved from 0.98730 to 0.99100, saving model to weights.best.hdf5
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.0016351559.
60000/60000 [==============================] - 7s 117us/step - loss: 0.1716 - acc: 0.9448 - val_loss: 0.0366 - val_acc: 0.9909

Epoch 00004: val_acc did not improve from 0.99100
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0014059754.
60000/60000 [==============================] - 7s 116us/step - loss: 0.1541 - acc: 0.9481 - val_loss: 0.0323 - val_acc: 0.9911

Epoch 00005: val_acc improved from 0.99100 to 0.99110, saving model to weights.best.hdf5
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0012331407.
60000/60000 [==============================] - 7s 117us/step - loss: 0.1414 - acc: 0.9509 - val_loss: 0.0316 - val_acc: 0.9915

Epoch 00006: val_acc improved from 0.99110 to 0.99150, saving model to weights.best.hdf5
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.0010981469.
60000/60000 [==============================] - 7s 116us/step - loss: 0.1310 - acc: 0.9518 - val_loss: 0.0271 - val_acc: 0.9933

Epoch 00007: val_acc improved from 0.99150 to 0.99330, saving model to weights.best.hdf5
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0009897928.
60000/60000 [==============================] - 7s 117us/step - loss: 0.1278 - acc: 0.9511 - val_loss: 0.0264 - val_acc: 0.9923

Epoch 00008: val_acc did not improve from 0.99330
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.0009009009.
60000/60000 [==============================] - 7s 116us/step - loss: 0.1202 - acc: 0.9532 - val_loss: 0.0252 - val_acc: 0.9937

Epoch 00009: val_acc improved from 0.99330 to 0.99370, saving model to weights.best.hdf5
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.0008266598.
60000/60000 [==============================] - 7s 116us/step - loss: 0.1140 - acc: 0.9549 - val_loss: 0.0247 - val_acc: 0.9928

Epoch 00010: val_acc did not improve from 0.99370
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.0007637232.
60000/60000 [==============================] - 7s 116us/step - loss: 0.1151 - acc: 0.9535 - val_loss: 0.0243 - val_acc: 0.9936

Epoch 00011: val_acc did not improve from 0.99370
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.0007096917.
60000/60000 [==============================] - 7s 117us/step - loss: 0.1104 - acc: 0.9549 - val_loss: 0.0226 - val_acc: 0.9935

Epoch 00012: val_acc did not improve from 0.99370
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.0006628003.
60000/60000 [==============================] - 7s 115us/step - loss: 0.1066 - acc: 0.9554 - val_loss: 0.0230 - val_acc: 0.9939

Epoch 00013: val_acc improved from 0.99370 to 0.99390, saving model to weights.best.hdf5
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.0006217214.
60000/60000 [==============================] - 7s 115us/step - loss: 0.1042 - acc: 0.9556 - val_loss: 0.0297 - val_acc: 0.9911

Epoch 00014: val_acc did not improve from 0.99390
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0005854372.
60000/60000 [==============================] - 7s 115us/step - loss: 0.1006 - acc: 0.9567 - val_loss: 0.0214 - val_acc: 0.9943

Epoch 00015: val_acc improved from 0.99390 to 0.99430, saving model to weights.best.hdf5
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0005531547.
60000/60000 [==============================] - 7s 116us/step - loss: 0.1006 - acc: 0.9568 - val_loss: 0.0216 - val_acc: 0.9940

Epoch 00016: val_acc did not improve from 0.99430
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.0005242464.
60000/60000 [==============================] - 7s 118us/step - loss: 0.1011 - acc: 0.9564 - val_loss: 0.0238 - val_acc: 0.9936

Epoch 00017: val_acc did not improve from 0.99430
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0004982096.
60000/60000 [==============================] - 7s 114us/step - loss: 0.1002 - acc: 0.9560 - val_loss: 0.0207 - val_acc: 0.9948

Epoch 00018: val_acc improved from 0.99430 to 0.99480, saving model to weights.best.hdf5
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0004746366.
60000/60000 [==============================] - 7s 114us/step - loss: 0.0970 - acc: 0.9566 - val_loss: 0.0207 - val_acc: 0.9943

Epoch 00019: val_acc did not improve from 0.99480
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.0004531936.
60000/60000 [==============================] - 7s 115us/step - loss: 0.0983 - acc: 0.9562 - val_loss: 0.0211 - val_acc: 0.9939

Epoch 00020: val_acc did not improve from 0.99480

<keras.callbacks.History at 0x7fe58789a240>


## Model Score on Test Data
[0.020666322801448405, 0.9948]

## Strategy Taken for Network Improvement
- Added Image AUgmentation method with horizontal shift and verticial shift as True
datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)
- Reduced Number of Filter into 3rd and 4th Convolution layer

- Increased LR marginally 
      Old:
      def scheduler(epoch, lr):
        return round(0.0032 * 1/(1 + 0.319 * epoch), 10)
     Revised:
       def scheduler(epoch, lr):
         return round(0.0032 * 1/(1 + 0.319 * epoch), 10)



