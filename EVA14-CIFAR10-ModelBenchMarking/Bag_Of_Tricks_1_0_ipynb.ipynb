{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZl71e8tk46H"
   },
   "source": [
    "### Set GPU clocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "5R_4K2s5k46J",
    "outputId": "5e6f4205-ea64-4d22-8789-9a16d46f231d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for gaurav: \n",
      "[sudo] password for gaurav: "
     ]
    }
   ],
   "source": [
    "!sudo nvidia-persistenced\n",
    "!sudo nvidia-smi -ac 877,1530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdUgS0HFk46L"
   },
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import singledispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ROzCHLyFmDpi"
   },
   "source": [
    "## Torch Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9vbaqGjk6rn"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# utils\n",
    "#####################\n",
    "\n",
    "class Timer():\n",
    "    def __init__(self, synch=None):\n",
    "        self.synch = synch or (lambda: None)\n",
    "        self.synch()\n",
    "        self.times = [time.time()]\n",
    "        self.total_time = 0.0\n",
    "\n",
    "    def __call__(self, include_in_total=True):\n",
    "        self.synch()\n",
    "        self.times.append(time.time())\n",
    "        delta_t = self.times[-1] - self.times[-2]\n",
    "        if include_in_total:\n",
    "            self.total_time += delta_t\n",
    "        return delta_t\n",
    "    \n",
    "localtime = lambda: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
    "\n",
    "class TableLogger():\n",
    "    def append(self, output):\n",
    "        if not hasattr(self, 'keys'):\n",
    "            self.keys = output.keys()\n",
    "            print(*(f'{k:>12s}' for k in self.keys))\n",
    "        filtered = [output[k] for k in self.keys]\n",
    "        print(*(f'{v:12.4f}' if isinstance(v, np.float) else f'{v:12}' for v in filtered))\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "## dict utils\n",
    "#####################\n",
    "\n",
    "union = lambda *dicts: {k: v for d in dicts for (k, v) in d.items()}\n",
    "\n",
    "def path_iter(nested_dict, pfx=()):\n",
    "    for name, val in nested_dict.items():\n",
    "        if isinstance(val, dict): yield from path_iter(val, (*pfx, name))\n",
    "        else: yield ((*pfx, name), val)  \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#####################\n",
    "## training utils\n",
    "#####################\n",
    "\n",
    "@singledispatch\n",
    "def cat(*xs):\n",
    "    raise NotImplementedError\n",
    "    \n",
    "@singledispatch\n",
    "def to_numpy(x):\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "class PiecewiseLinear(namedtuple('PiecewiseLinear', ('knots', 'vals'))):\n",
    "    def __call__(self, t):\n",
    "        return np.interp([t], self.knots, self.vals)[0]\n",
    "\n",
    "class StatsLogger():\n",
    "    def __init__(self, keys):\n",
    "        self._stats = {k:[] for k in keys}\n",
    "\n",
    "    def append(self, output):\n",
    "        for k,v in self._stats.items():\n",
    "            v.append(output[k].detach())\n",
    "    \n",
    "    def stats(self, key):\n",
    "        return cat(*self._stats[key])\n",
    "        \n",
    "    def mean(self, key):\n",
    "        return np.mean(to_numpy(self.stats(key)), dtype=np.float)\n",
    "\n",
    "def run_batches(model, batches, training, optimizer_step=None, stats=None):\n",
    "    stats = stats or StatsLogger(('loss', 'correct'))\n",
    "    model.train(training)   \n",
    "    for batch in batches:\n",
    "        output = model(batch)\n",
    "        stats.append(output) \n",
    "        if training:\n",
    "            output['loss'].sum().backward()\n",
    "            optimizer_step()\n",
    "            model.zero_grad() \n",
    "    return stats\n",
    "    \n",
    "def train_epoch(model, train_batches, test_batches, optimizer_step, timer, test_time_in_total=True):\n",
    "    train_stats, train_time = run_batches(model, train_batches, True, optimizer_step), timer()\n",
    "    test_stats, test_time = run_batches(model, test_batches, False), timer(test_time_in_total)\n",
    "    return { \n",
    "        'train time': train_time, 'train loss': train_stats.mean('loss'), 'train acc': train_stats.mean('correct'), \n",
    "        'test time': test_time, 'test loss': test_stats.mean('loss'), 'test acc': test_stats.mean('correct'),\n",
    "        'total time': timer.total_time, \n",
    "    }\n",
    "\n",
    "def train(model, optimizer, train_batches, test_batches, epochs, \n",
    "          loggers=(), test_time_in_total=True, timer=None):  \n",
    "    timer = timer or Timer()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_stats = train_epoch(model, train_batches, test_batches, optimizer.step, timer, test_time_in_total=test_time_in_total) \n",
    "        summary = union({'epoch': epoch+1, 'lr': optimizer.param_values()['lr']*train_batches.batch_size}, epoch_stats)\n",
    "        for logger in loggers:\n",
    "            logger.append(summary)    \n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HF8ZX2DVmIvi"
   },
   "source": [
    "## Data Augmentation Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCGoHFeImV2V"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "## data augmentation\n",
    "#####################\n",
    "\n",
    "class Crop(namedtuple('Crop', ('h', 'w'))):\n",
    "    def __call__(self, x, x0, y0):\n",
    "        return x[:,y0:y0+self.h,x0:x0+self.w]\n",
    "\n",
    "    def options(self, x_shape):\n",
    "        C, H, W = x_shape\n",
    "        return {'x0': range(W+1-self.w), 'y0': range(H+1-self.h)}\n",
    "    \n",
    "    def output_shape(self, x_shape):\n",
    "        C, H, W = x_shape\n",
    "        return (C, self.h, self.w)\n",
    "    \n",
    "class FlipLR(namedtuple('FlipLR', ())):\n",
    "    def __call__(self, x, choice):\n",
    "        return x[:, :, ::-1].copy() if choice else x \n",
    "        \n",
    "    def options(self, x_shape):\n",
    "        return {'choice': [True, False]}\n",
    "\n",
    "class Cutout(namedtuple('Cutout', ('h', 'w'))):\n",
    "    def __call__(self, x, x0, y0):\n",
    "        x = x.copy()\n",
    "        x[:,y0:y0+self.h,x0:x0+self.w].fill(0.0)\n",
    "        return x\n",
    "\n",
    "    def options(self, x_shape):\n",
    "        C, H, W = x_shape\n",
    "        return {'x0': range(W+1-self.w), 'y0': range(H+1-self.h)} \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAVAHFPjmks7"
   },
   "source": [
    "## Data Preprocessing Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-aRoV3QmnLp"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "## data preprocessing\n",
    "#####################\n",
    "\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465) # equals np.mean(train_set.train_data, axis=(0,1,2))/255\n",
    "cifar10_std = (0.2471, 0.2435, 0.2616) # equals np.std(train_set.train_data, axis=(0,1,2))/255\n",
    "\n",
    "def normalise(x, mean=cifar10_mean, std=cifar10_std):\n",
    "    x, mean, std = [np.array(a, np.float32) for a in (x, mean, std)]\n",
    "    x -= mean*255\n",
    "    x *= 1.0/(255*std)\n",
    "    return x\n",
    "\n",
    "def pad(x, border=4):\n",
    "    return np.pad(x, [(0, 0), (border, border), (border, border), (0, 0)], mode='reflect')\n",
    "\n",
    "def transpose(x, source='NHWC', target='NCHW'):\n",
    "    return x.transpose([source.index(d) for d in target]) \n",
    "\n",
    "    \n",
    "class Transform():\n",
    "    def __init__(self, dataset, transforms):\n",
    "        self.dataset, self.transforms = dataset, transforms\n",
    "        self.choices = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "           \n",
    "    def __getitem__(self, index):\n",
    "        data, labels = self.dataset[index]\n",
    "        for choices, f in zip(self.choices, self.transforms):\n",
    "            args = {k: v[index] for (k,v) in choices.items()}\n",
    "            data = f(data, **args)\n",
    "        return data, labels\n",
    "    \n",
    "    def set_random_choices(self):\n",
    "        self.choices = []\n",
    "        x_shape = self.dataset[0][0].shape\n",
    "        N = len(self)\n",
    "        for t in self.transforms:\n",
    "            options = t.options(x_shape)\n",
    "            x_shape = t.output_shape(x_shape) if hasattr(t, 'output_shape') else x_shape\n",
    "            self.choices.append({k:np.random.choice(v, size=N) for (k,v) in options.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-9CyxYqmxJh"
   },
   "source": [
    "## Model Graph Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkbNq1ommx4f"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "## graph building\n",
    "#####################\n",
    "\n",
    "sep='_'\n",
    "RelativePath = namedtuple('RelativePath', ('parts'))\n",
    "rel_path = lambda *parts: RelativePath(parts)\n",
    "\n",
    "def build_graph(net):\n",
    "    net = dict(path_iter(net)) \n",
    "    default_inputs = [[('input',)]]+[[k] for k in net.keys()]\n",
    "    with_default_inputs = lambda vals: (val if isinstance(val, tuple) else (val, default_inputs[idx]) for idx,val in enumerate(vals))\n",
    "    parts = lambda path, pfx: tuple(pfx) + path.parts if isinstance(path, RelativePath) else (path,) if isinstance(path, str) else path\n",
    "    return {sep.join((*pfx, name)): (val, [sep.join(parts(x, pfx)) for x in inputs]) for (*pfx, name), (val, inputs) in zip(net.keys(), with_default_inputs(net.values()))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcHfjNw1m-V7"
   },
   "source": [
    "## Model Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hL851_vpnALF"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "## network visualisation (requires pydot)\n",
    "#####################\n",
    "class ColorMap(dict):\n",
    "    palette = (\n",
    "        'bebada,ffffb3,fb8072,8dd3c7,80b1d3,fdb462,b3de69,fccde5,bc80bd,ccebc5,ffed6f,1f78b4,33a02c,e31a1c,ff7f00,'\n",
    "        '4dddf8,e66493,b07b87,4e90e3,dea05e,d0c281,f0e189,e9e8b1,e0eb71,bbd2a4,6ed641,57eb9c,3ca4d4,92d5e7,b15928'\n",
    "    ).split(',')\n",
    "    def __missing__(self, key):\n",
    "        self[key] = self.palette[len(self) % len(self.palette)]\n",
    "        return self[key]\n",
    "\n",
    "def make_pydot(nodes, edges, direction='LR', sep=sep, **kwargs):\n",
    "    import pydot\n",
    "    parent = lambda path: path[:-1]\n",
    "    stub = lambda path: path[-1]\n",
    "    class Subgraphs(dict):\n",
    "        def __missing__(self, path):\n",
    "            subgraph = pydot.Cluster(sep.join(path), label=stub(path), style='rounded, filled', fillcolor='#77777744')\n",
    "            self[parent(path)].add_subgraph(subgraph)\n",
    "            return subgraph\n",
    "    subgraphs = Subgraphs()\n",
    "    subgraphs[()] = g = pydot.Dot(rankdir=direction, directed=True, **kwargs)\n",
    "    g.set_node_defaults(\n",
    "        shape='box', style='rounded, filled', fillcolor='#ffffff')\n",
    "    for node, attr in nodes:\n",
    "        path = tuple(node.split(sep))\n",
    "        subgraphs[parent(path)].add_node(\n",
    "            pydot.Node(name=node, label=stub(path), **attr))\n",
    "    for src, dst, attr in edges:\n",
    "        g.add_edge(pydot.Edge(src, dst, **attr))\n",
    "    return g\n",
    "\n",
    "get_params = lambda mod: {p.name: getattr(mod, p.name, '?') for p in signature(type(mod)).parameters.values()}\n",
    "\n",
    "\n",
    "class DotGraph():\n",
    "    colors = ColorMap()\n",
    "    def __init__(self, net, size=15, direction='LR'):\n",
    "        graph = build_graph(net)\n",
    "        self.nodes = [(k, {\n",
    "            'tooltip': '%s %.1000r' % (type(n).__name__, get_params(n)), \n",
    "            'fillcolor': '#'+self.colors[type(n)],\n",
    "        }) for k, (n, i) in graph.items()] \n",
    "        self.edges = [(src, k, {}) for (k, (n, i)) in graph.items() for src in i]\n",
    "        self.size, self.direction = size, direction\n",
    "\n",
    "    def dot_graph(self, **kwargs):\n",
    "        return make_pydot(self.nodes, self.edges, size=self.size, \n",
    "                            direction=self.direction, **kwargs)\n",
    "\n",
    "    def svg(self, **kwargs):\n",
    "        return self.dot_graph(**kwargs).create(format='svg').decode('utf-8')\n",
    "\n",
    "    try:\n",
    "        import pydot\n",
    "        def _repr_svg_(self):\n",
    "            return self.svg()\n",
    "    except ImportError:\n",
    "        def __repr__(self):\n",
    "            return 'pydot is needed for network visualisation'\n",
    "\n",
    "walk = lambda dict_, key: walk(dict_, dict_[key]) if key in dict_ else key\n",
    "   \n",
    "def remove_by_type(net, node_type):  \n",
    "    #remove identity nodes for more compact visualisations\n",
    "    graph = build_graph(net)\n",
    "    remap = {k: i[0] for k,(v,i) in graph.items() if isinstance(v, node_type)}\n",
    "    return {k: (v, [walk(remap, x) for x in i]) for k, (v,i) in graph.items() if not isinstance(v, node_type)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CoyTj72NmWot"
   },
   "source": [
    "## Model Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6cq73qllJOQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "# from core import build_graph, cat, to_numpy\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@cat.register(torch.Tensor)\n",
    "def _(*xs):\n",
    "    return torch.cat(xs)\n",
    "\n",
    "@to_numpy.register(torch.Tensor)\n",
    "def _(x):\n",
    "    return x.detach().cpu().numpy()  \n",
    "\n",
    "def warmup_cudnn(model, batch_size):\n",
    "    #run forward and backward pass of the model on a batch of random inputs\n",
    "    #to allow benchmarking of cudnn kernels \n",
    "    batch = {\n",
    "        'input': torch.Tensor(np.random.rand(batch_size,3,32,32)).cuda().half(), \n",
    "        'target': torch.LongTensor(np.random.randint(0,10,batch_size)).cuda()\n",
    "    }\n",
    "    model.train(True)\n",
    "    o = model(batch)\n",
    "    o['loss'].sum().backward()\n",
    "    model.zero_grad()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "#####################\n",
    "## dataset\n",
    "#####################\n",
    "\n",
    "def cifar10(root):\n",
    "    train_set = torchvision.datasets.CIFAR10(root=root, train=True, download=True)\n",
    "    test_set = torchvision.datasets.CIFAR10(root=root, train=False, download=True)\n",
    "    return {\n",
    "        'train': {'data': train_set.data, 'labels': train_set.targets},\n",
    "        'test': {'data': test_set.data, 'labels': test_set.targets}\n",
    "    }\n",
    "\n",
    "#####################\n",
    "## data loading\n",
    "#####################\n",
    "\n",
    "class Batches():\n",
    "    def __init__(self, dataset, batch_size, shuffle, set_random_choices=False, num_workers=0, drop_last=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.set_random_choices = set_random_choices\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=shuffle, drop_last=drop_last\n",
    "        )\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.set_random_choices:\n",
    "            self.dataset.set_random_choices() \n",
    "        return ({'input': x.to(device).half(), 'target': y.to(device).long()} for (x,y) in self.dataloader)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.dataloader)\n",
    "\n",
    "#####################\n",
    "## torch stuff\n",
    "#####################\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, x): return x\n",
    "    \n",
    "class Mul(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    def __call__(self, x): \n",
    "        return x*self.weight\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), x.size(1))\n",
    "\n",
    "class Add(nn.Module):\n",
    "    def forward(self, x, y): return x + y \n",
    "    \n",
    "class Concat(nn.Module):\n",
    "    def forward(self, *xs): return torch.cat(xs, 1)\n",
    "    \n",
    "class Correct(nn.Module):\n",
    "    def forward(self, classifier, target):\n",
    "        return classifier.max(dim = 1)[1] == target\n",
    "\n",
    "def batch_norm(num_channels, bn_bias_init=None, bn_bias_freeze=False, bn_weight_init=None, bn_weight_freeze=False):\n",
    "    m = nn.BatchNorm2d(num_channels)\n",
    "    if bn_bias_init is not None:\n",
    "        m.bias.data.fill_(bn_bias_init)\n",
    "    if bn_bias_freeze:\n",
    "        m.bias.requires_grad = False\n",
    "    if bn_weight_init is not None:\n",
    "        m.weight.data.fill_(bn_weight_init)\n",
    "    if bn_weight_freeze:\n",
    "        m.weight.requires_grad = False\n",
    "        \n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        self.graph = build_graph(net)\n",
    "        super().__init__()\n",
    "        for n, (v, _) in self.graph.items(): \n",
    "            setattr(self, n, v)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.cache = dict(inputs)\n",
    "        for n, (_, i) in self.graph.items():\n",
    "            self.cache[n] = getattr(self, n)(*[self.cache[x] for x in i])\n",
    "        return self.cache\n",
    "    \n",
    "    def half(self):\n",
    "        for module in self.children():\n",
    "            if not isinstance(module, nn.BatchNorm2d):\n",
    "                module.half()    \n",
    "        return self\n",
    "\n",
    "trainable_params = lambda model:filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "class TorchOptimiser():\n",
    "    def __init__(self, weights, optimizer, step_number=0, **opt_params):\n",
    "        self.weights = weights\n",
    "        self.step_number = step_number\n",
    "        self.opt_params = opt_params\n",
    "        self._opt = optimizer(weights, **self.param_values())\n",
    "    \n",
    "    def param_values(self):\n",
    "        return {k: v(self.step_number) if callable(v) else v for k,v in self.opt_params.items()}\n",
    "    \n",
    "    def step(self):\n",
    "        self.step_number += 1\n",
    "        self._opt.param_groups[0].update(**self.param_values())\n",
    "        self._opt.step()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._opt)\n",
    "        \n",
    "def SGD(weights, lr=0, momentum=0, weight_decay=0, dampening=0, nesterov=False):\n",
    "    return TorchOptimiser(weights, torch.optim.SGD, lr=lr, momentum=momentum, \n",
    "                          weight_decay=weight_decay, dampening=dampening, \n",
    "                          nesterov=nesterov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyOJKmd_k46N"
   },
   "source": [
    "### Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "KjLoIrvCk46O",
    "outputId": "149290cc-d27d-4e48-bd05-b3f3c44b4a73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "def conv_bn(c_in, c_out, bn_weight_init=1.0, **kw):\n",
    "    return {\n",
    "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
    "        'bn': batch_norm(c_out, bn_weight_init=bn_weight_init, **kw), \n",
    "        'relu': nn.ReLU(True)\n",
    "    }\n",
    "\n",
    "def residual(c, **kw):\n",
    "    return {\n",
    "        'in': Identity(),\n",
    "        'res1': conv_bn(c, c, **kw),\n",
    "        'res2': conv_bn(c, c, **kw),\n",
    "        'add': (Add(), [rel_path('in'), rel_path('res2', 'relu')]),\n",
    "    }\n",
    "\n",
    "def basic_net(channels, weight,  pool, **kw):\n",
    "    return {\n",
    "        'prep': conv_bn(3, channels['prep'], **kw),\n",
    "        'layer1': dict(conv_bn(channels['prep'], channels['layer1'], **kw), pool=pool),\n",
    "        'layer2': dict(conv_bn(channels['layer1'], channels['layer2'], **kw), pool=pool),\n",
    "        'layer3': dict(conv_bn(channels['layer2'], channels['layer3'], **kw), pool=pool),\n",
    "\n",
    "        'pool': nn.MaxPool2d(4),\n",
    "        'flatten': Flatten(),\n",
    "        'linear': nn.Linear(channels['layer3'], 10, bias=False),\n",
    "        'classifier': Mul(weight),\n",
    "    }\n",
    "\n",
    "def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3'), **kw):\n",
    "    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512}\n",
    "    n = basic_net(channels, weight, pool, **kw)\n",
    "    for layer in res_layers:\n",
    "        n[layer]['residual'] = residual(channels[layer], **kw)\n",
    "    for layer in extra_layers:\n",
    "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer], **kw)       \n",
    "    return n\n",
    "\n",
    "losses = {\n",
    "    'loss':  (nn.CrossEntropyLoss(reduce=False), [('classifier',), ('target',)]),\n",
    "    'correct': (Correct(), [('classifier',), ('target',)]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qr43TLnjk46R"
   },
   "source": [
    "### Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "2J_y_Opek46S",
    "outputId": "81a46f4c-c34f-4613-8e3a-a1b06e3f970d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Preprocessing training data\n",
      "Finished in 1.8 seconds\n",
      "Preprocessing test data\n",
      "Finished in 0.077 seconds\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data'\n",
    "dataset = cifar10(root=DATA_DIR)\n",
    "t = Timer()\n",
    "print('Preprocessing training data')\n",
    "train_set = list(zip(transpose(normalise(pad(dataset['train']['data'], 4))), dataset['train']['labels']))\n",
    "print(f'Finished in {t():.2} seconds')\n",
    "print('Preprocessing test data')\n",
    "test_set = list(zip(transpose(normalise(dataset['test']['data'])), dataset['test']['labels']))\n",
    "print(f'Finished in {t():.2} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wjhDcyTrk46W"
   },
   "source": [
    "### Network visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "d9VSCE2bk46W",
    "outputId": "e424f645-aa7c-4be9-c8cf-bbf05283e2a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydot is needed for network visualisation"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(DotGraph(net()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXp9J7x9k46Z"
   },
   "source": [
    "### Training\n",
    "\n",
    "NB: on the first run, the first epoch will be slower as initialisation and Cudnn benchmarking take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FaNu3QCFk46a",
    "outputId": "6673fcf2-fadb-41cc-9603-182af575f555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Run 0 at 2019-08-22 09:03:58\n",
      "       epoch           lr   train time   train loss    train acc    test time    test loss     test acc   total time\n",
      "           1       0.0800       7.2463       1.6233       0.4166       0.7044       1.5635       0.5022       7.9507\n",
      "           2       0.1600       5.5980       0.9324       0.6659       0.3828       0.9080       0.6932      13.9315\n",
      "           3       0.2400       5.5540       0.7193       0.7498       0.3839       0.8253       0.7122      19.8694\n",
      "           4       0.3200       5.5090       0.6198       0.7830       0.3804       0.6895       0.7740      25.7588\n",
      "           5       0.4000       5.5088       0.5492       0.8094       0.3831       0.6727       0.7676      31.6508\n",
      "           6       0.3789       5.5648       0.4953       0.8300       0.3853       0.6484       0.7814      37.6008\n",
      "           7       0.3579       5.6548       0.4463       0.8463       0.3859       0.6638       0.7734      43.6415\n",
      "           8       0.3368       5.5248       0.4094       0.8594       0.3849       0.4472       0.8467      49.5512\n",
      "           9       0.3158       5.5372       0.3835       0.8695       0.3867       0.6188       0.7977      55.4750\n",
      "          10       0.2947       5.5499       0.3642       0.8753       0.3864       0.4878       0.8382      61.4114\n",
      "          11       0.2737       5.5492       0.3398       0.8838       0.3862       0.4597       0.8495      67.3468\n",
      "          12       0.2526       5.5546       0.3264       0.8897       0.3865       0.4307       0.8557      73.2879\n",
      "          13       0.2316       5.6289       0.3053       0.8969       0.3875       0.4000       0.8682      79.3043\n",
      "          14       0.2105       5.6372       0.2842       0.9034       0.3880       0.6152       0.8023      85.3294\n",
      "          15       0.1895       5.6430       0.2622       0.9104       0.3886       0.4827       0.8421      91.3610\n",
      "          16       0.1684       5.5867       0.2479       0.9152       0.4129       0.3803       0.8718      97.3606\n",
      "          17       0.1474       5.5879       0.2304       0.9233       0.3897       0.3155       0.8931     103.3382\n",
      "          18       0.1263       5.5893       0.2072       0.9296       0.3895       0.3298       0.8883     109.3169\n",
      "          19       0.1053       5.5859       0.1851       0.9376       0.3893       0.2830       0.9027     115.2921\n",
      "          20       0.0842       5.6554       0.1651       0.9461       0.4147       0.3064       0.9013     121.3623\n",
      "          21       0.0632       5.5874       0.1398       0.9547       0.3891       0.2370       0.9202     127.3388\n",
      "          22       0.0421       5.6728       0.1136       0.9647       0.3893       0.2193       0.9247     133.4008\n",
      "          23       0.0211       5.5901       0.0910       0.9726       0.3894       0.1852       0.9369     139.3803\n",
      "          24       0.0000       5.5884       0.0747       0.9781       0.4133       0.1764       0.9424     145.3820\n",
      "Starting Run 1 at 2019-08-22 09:06:27\n",
      "       epoch           lr   train time   train loss    train acc    test time    test loss     test acc   total time\n",
      "           1       0.0800       5.5945       1.6146       0.4171       0.3904       2.5556       0.3257       5.9849\n",
      "           2       0.1600       5.9676       0.9202       0.6727       0.3901       0.9105       0.6972      12.3425\n",
      "           3       0.2400       5.6777       0.7165       0.7504       0.3907       0.6973       0.7599      18.4109\n",
      "           4       0.3200       6.0472       0.6163       0.7857       0.4148       1.4174       0.5919      24.8729\n",
      "           5       0.4000       6.1268       0.5596       0.8038       0.4431       0.6223       0.7788      31.4428\n",
      "           6       0.3789       6.0205       0.5004       0.8260       0.3929       0.5718       0.8078      37.8562\n",
      "           7       0.3579       6.0633       0.4425       0.8479       0.3896       0.4741       0.8332      44.3091\n",
      "           8       0.3368       6.4823       0.4105       0.8600       0.4232       0.4791       0.8383      51.2145\n",
      "           9       0.3158       6.6284       0.3845       0.8686       0.3895       0.6456       0.7849      58.2324\n",
      "          10       0.2947       6.2754       0.3619       0.8770       0.3911       0.3881       0.8675      64.8989\n",
      "          11       0.2737       5.6160       0.3388       0.8851       0.3905       0.5071       0.8265      70.9054\n",
      "          12       0.2526       5.9404       0.3298       0.8887       0.3877       0.4472       0.8520      77.2335\n",
      "          13       0.2316       5.7435       0.3033       0.8960       0.3910       0.4206       0.8602      83.3680\n",
      "          14       0.2105       5.6116       0.2927       0.9019       0.3909       0.3783       0.8767      89.3705\n",
      "          15       0.1895       5.6136       0.2700       0.9082       0.3910       0.4006       0.8650      95.3751\n",
      "          16       0.1684       5.9513       0.2497       0.9143       0.3916       0.3496       0.8803     101.7180\n",
      "          17       0.1474       6.3654       0.2298       0.9224       0.4608       0.3517       0.8792     108.5442\n",
      "          18       0.1263       5.8767       0.2090       0.9293       0.3907       0.3953       0.8688     114.8116\n",
      "          19       0.1053       5.6122       0.1881       0.9368       0.3904       0.3413       0.8884     120.8141\n",
      "          20       0.0842       5.6129       0.1627       0.9475       0.3906       0.2898       0.9011     126.8176\n",
      "          21       0.0632       5.6582       0.1392       0.9547       0.3905       0.2493       0.9176     132.8663\n",
      "          22       0.0421       5.6118       0.1148       0.9642       0.3905       0.2115       0.9310     138.8686\n",
      "          23       0.0211       5.6135       0.0921       0.9727       0.3904       0.1984       0.9348     144.8725\n",
      "          24       0.0000       5.6138       0.0769       0.9781       0.3903       0.1801       0.9420     150.8766\n",
      "Starting Run 2 at 2019-08-22 09:08:58\n",
      "       epoch           lr   train time   train loss    train acc    test time    test loss     test acc   total time\n",
      "           1       0.0800       5.6339       1.6200       0.4151       0.3949       1.3507       0.5174       6.0288\n",
      "           2       0.1600       5.6316       0.9431       0.6644       0.3950       0.7972       0.7202      12.0554\n",
      "           3       0.2400       5.6235       0.7329       0.7443       0.3949       0.6101       0.7870      18.0737\n",
      "           4       0.3200       5.6214       0.6286       0.7816       0.3953       0.7346       0.7518      24.0905\n",
      "           5       0.4000       5.6172       0.5550       0.8081       0.3951       0.5833       0.7998      30.1028\n",
      "           6       0.3789       5.6188       0.4959       0.8287       0.3950       0.5198       0.8175      36.1165\n",
      "           7       0.3579       5.6199       0.4472       0.8459       0.3945       0.5326       0.8134      42.1309\n",
      "           8       0.3368       5.6138       0.4076       0.8621       0.3945       0.6789       0.7757      48.1392\n",
      "           9       0.3158       5.6140       0.3882       0.8670       0.3952       0.4306       0.8552      54.1485\n",
      "          10       0.2947       5.6120       0.3633       0.8756       0.3942       0.4958       0.8355      60.1547\n",
      "          11       0.2737       5.6135       0.3438       0.8831       0.3940       0.4851       0.8359      66.1621\n",
      "          12       0.2526       5.6150       0.3236       0.8908       0.3916       0.4337       0.8475      72.1686\n",
      "          13       0.2316       5.6113       0.3044       0.8967       0.3911       0.3575       0.8816      78.1710\n",
      "          14       0.2105       5.6120       0.2916       0.9003       0.3911       0.4926       0.8324      84.1741\n",
      "          15       0.1895       5.6126       0.2660       0.9099       0.3912       0.4220       0.8606      90.1779\n",
      "          16       0.1684       5.6135       0.2510       0.9167       0.3904       0.4921       0.8269      96.1818\n",
      "          17       0.1474       5.6580       0.2307       0.9227       0.3904       0.3018       0.9005     102.2302\n",
      "          18       0.1263       5.6131       0.2108       0.9305       0.3906       0.3241       0.8872     108.2339\n",
      "          19       0.1053       5.6121       0.1843       0.9391       0.3909       0.2896       0.9024     114.2369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          20       0.0842       5.6121       0.1601       0.9472       0.3912       0.2801       0.9070     120.2403\n",
      "          21       0.0632       5.6134       0.1388       0.9551       0.3912       0.2464       0.9189     126.2448\n",
      "          22       0.0421       5.6101       0.1147       0.9640       0.3911       0.2101       0.9313     132.2460\n",
      "          23       0.0211       5.6102       0.0939       0.9715       0.3909       0.1898       0.9373     138.2471\n",
      "          24       0.0000       5.6095       0.0772       0.9773       0.3912       0.1797       0.9411     144.2478\n",
      "mean test accuracy: 0.9418\n",
      "median test accuracy: 0.9420\n",
      "3/3 >= 94%\n"
     ]
    }
   ],
   "source": [
    "epochs=24\n",
    "lr_schedule = PiecewiseLinear([0, 5, epochs], [0, 0.4, 0])\n",
    "batch_size = 512\n",
    "transforms = [Crop(32, 32), FlipLR(), Cutout(8, 8)]\n",
    "N_runs = 3\n",
    "\n",
    "train_batches = Batches(Transform(train_set, transforms), batch_size, shuffle=True, set_random_choices=True, drop_last=True)\n",
    "test_batches = Batches(test_set, batch_size, shuffle=False, drop_last=False)\n",
    "lr = lambda step: lr_schedule(step/len(train_batches))/batch_size\n",
    "\n",
    "summaries = []\n",
    "for i in range(N_runs):\n",
    "    print(f'Starting Run {i} at {localtime()}')\n",
    "    model = Network(union(net(), losses)).to(device).half()\n",
    "    opt = SGD(trainable_params(model), lr=lr, momentum=0.9, weight_decay=5e-4*batch_size, nesterov=True)\n",
    "    summaries.append(train(model, opt, train_batches, test_batches, epochs, loggers=(TableLogger(),)))\n",
    "\n",
    "test_accs = np.array([s['test acc'] for s in summaries])\n",
    "print(f'mean test accuracy: {np.mean(test_accs):.4f}')\n",
    "print(f'median test accuracy: {np.median(test_accs):.4f}')\n",
    "print(f'{np.sum(test_accs>=0.94)}/{N_runs} >= 94%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vllLdhINk46c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EVA_Asgnmnt_14_Bag_Of_Tricks_1.0.ipynb.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
