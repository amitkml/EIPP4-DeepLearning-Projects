## Baseline Model log

Epoch 1/50
390/390 [==============================] - 19s 48ms/step - loss: 1.8634 - acc: 0.2853 - val_loss: 1.5148 - val_acc: 0.4386
Epoch 2/50
390/390 [==============================] - 8s 20ms/step - loss: 1.4027 - acc: 0.4860 - val_loss: 1.1998 - val_acc: 0.5703
Epoch 3/50
390/390 [==============================] - 8s 20ms/step - loss: 1.1777 - acc: 0.5821 - val_loss: 1.0308 - val_acc: 0.6368
Epoch 4/50
390/390 [==============================] - 8s 20ms/step - loss: 1.0224 - acc: 0.6439 - val_loss: 0.9538 - val_acc: 0.6636
Epoch 5/50
390/390 [==============================] - 8s 20ms/step - loss: 0.9232 - acc: 0.6780 - val_loss: 0.8299 - val_acc: 0.7131
Epoch 6/50
390/390 [==============================] - 8s 20ms/step - loss: 0.8387 - acc: 0.7097 - val_loss: 0.7559 - val_acc: 0.7405
Epoch 7/50
390/390 [==============================] - 8s 20ms/step - loss: 0.7815 - acc: 0.7312 - val_loss: 0.7509 - val_acc: 0.7439
Epoch 8/50
390/390 [==============================] - 8s 20ms/step - loss: 0.7299 - acc: 0.7499 - val_loss: 0.7800 - val_acc: 0.7348
Epoch 9/50
390/390 [==============================] - 8s 20ms/step - loss: 0.6847 - acc: 0.7657 - val_loss: 0.6620 - val_acc: 0.7753
Epoch 10/50
390/390 [==============================] - 8s 20ms/step - loss: 0.6507 - acc: 0.7784 - val_loss: 0.6480 - val_acc: 0.7784
Epoch 11/50
390/390 [==============================] - 8s 20ms/step - loss: 0.6238 - acc: 0.7868 - val_loss: 0.6286 - val_acc: 0.7855
Epoch 12/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5974 - acc: 0.7971 - val_loss: 0.6494 - val_acc: 0.7803
Epoch 13/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5776 - acc: 0.8040 - val_loss: 0.6414 - val_acc: 0.7827
Epoch 14/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5521 - acc: 0.8128 - val_loss: 0.6094 - val_acc: 0.7939
Epoch 15/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5355 - acc: 0.8184 - val_loss: 0.6295 - val_acc: 0.7898
Epoch 16/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5220 - acc: 0.8227 - val_loss: 0.5882 - val_acc: 0.8042
Epoch 17/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5126 - acc: 0.8254 - val_loss: 0.6110 - val_acc: 0.8006
Epoch 18/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5016 - acc: 0.8281 - val_loss: 0.5853 - val_acc: 0.8075
Epoch 19/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4856 - acc: 0.8341 - val_loss: 0.5924 - val_acc: 0.8031
Epoch 20/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4783 - acc: 0.8362 - val_loss: 0.6104 - val_acc: 0.7990
Epoch 21/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4623 - acc: 0.8409 - val_loss: 0.5820 - val_acc: 0.8179
Epoch 22/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4410 - acc: 0.8501 - val_loss: 0.6177 - val_acc: 0.7998
Epoch 23/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4361 - acc: 0.8518 - val_loss: 0.6214 - val_acc: 0.8003
Epoch 24/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4356 - acc: 0.8501 - val_loss: 0.5838 - val_acc: 0.8172
Epoch 25/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4196 - acc: 0.8556 - val_loss: 0.5560 - val_acc: 0.8205
Epoch 26/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4189 - acc: 0.8593 - val_loss: 0.5949 - val_acc: 0.8096
Epoch 27/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4156 - acc: 0.8577 - val_loss: 0.5723 - val_acc: 0.8159
Epoch 28/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4066 - acc: 0.8609 - val_loss: 0.5921 - val_acc: 0.8166
Epoch 29/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3957 - acc: 0.8656 - val_loss: 0.6035 - val_acc: 0.8090
Epoch 30/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4018 - acc: 0.8642 - val_loss: 0.5739 - val_acc: 0.8190
Epoch 31/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3839 - acc: 0.8695 - val_loss: 0.5889 - val_acc: 0.8191
Epoch 32/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3805 - acc: 0.8712 - val_loss: 0.5799 - val_acc: 0.8186
Epoch 33/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3822 - acc: 0.8702 - val_loss: 0.6240 - val_acc: 0.8064
Epoch 34/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3721 - acc: 0.8746 - val_loss: 0.5895 - val_acc: 0.8147
Epoch 35/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3662 - acc: 0.8755 - val_loss: 0.5635 - val_acc: 0.8210
Epoch 36/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3686 - acc: 0.8746 - val_loss: 0.5714 - val_acc: 0.8220
Epoch 37/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3608 - acc: 0.8780 - val_loss: 0.5979 - val_acc: 0.8131
Epoch 38/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3636 - acc: 0.8788 - val_loss: 0.5871 - val_acc: 0.8206
Epoch 39/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3508 - acc: 0.8825 - val_loss: 0.5780 - val_acc: 0.8211
Epoch 40/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3564 - acc: 0.8813 - val_loss: 0.5899 - val_acc: 0.8223
Epoch 41/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3425 - acc: 0.8834 - val_loss: 0.5839 - val_acc: 0.8201
Epoch 42/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3473 - acc: 0.8834 - val_loss: 0.6109 - val_acc: 0.8123
Epoch 43/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3377 - acc: 0.8845 - val_loss: 0.5778 - val_acc: 0.8230
Epoch 44/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3347 - acc: 0.8877 - val_loss: 0.5740 - val_acc: 0.8203
Epoch 45/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3378 - acc: 0.8870 - val_loss: 0.5794 - val_acc: 0.8287
Epoch 46/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3301 - acc: 0.8886 - val_loss: 0.6280 - val_acc: 0.8116
Epoch 47/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3321 - acc: 0.8891 - val_loss: 0.6077 - val_acc: 0.8185
Epoch 48/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3244 - acc: 0.8927 - val_loss: 0.6317 - val_acc: 0.8179
Epoch 49/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3293 - acc: 0.8896 - val_loss: 0.5761 - val_acc: 0.8254
Epoch 50/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3232 - acc: 0.8919 - val_loss: 0.5906 - val_acc: 0.8245
Model took 397.36 seconds to train

## Depthwise Separable Convolution Log

Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.032.
390/390 [==============================] - 61s 157ms/step - loss: 1.5698 - acc: 0.4127 - val_loss: 4.3453 - val_acc: 0.3043

Epoch 00001: val_acc improved from -inf to 0.30430, saving model to weights_a3_5B.best.hdf5
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0262510254.
390/390 [==============================] - 56s 143ms/step - loss: 1.1465 - acc: 0.5868 - val_loss: 1.9577 - val_acc: 0.5078

Epoch 00002: val_acc improved from 0.30430 to 0.50780, saving model to weights_a3_5B.best.hdf5
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0222531293.
390/390 [==============================] - 56s 142ms/step - loss: 0.9530 - acc: 0.6603 - val_loss: 1.5015 - val_acc: 0.5322

Epoch 00003: val_acc improved from 0.50780 to 0.53220, saving model to weights_a3_5B.best.hdf5
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.0193120097.
390/390 [==============================] - 56s 143ms/step - loss: 0.8410 - acc: 0.7034 - val_loss: 1.4065 - val_acc: 0.5736

Epoch 00004: val_acc improved from 0.53220 to 0.57360, saving model to weights_a3_5B.best.hdf5
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0170575693.
390/390 [==============================] - 56s 142ms/step - loss: 0.7583 - acc: 0.7338 - val_loss: 1.0983 - val_acc: 0.6529

Epoch 00005: val_acc improved from 0.57360 to 0.65290, saving model to weights_a3_5B.best.hdf5
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.015274463.
390/390 [==============================] - 56s 143ms/step - loss: 0.7001 - acc: 0.7539 - val_loss: 0.8787 - val_acc: 0.7027

Epoch 00006: val_acc improved from 0.65290 to 0.70270, saving model to weights_a3_5B.best.hdf5
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.0138288678.
390/390 [==============================] - 56s 143ms/step - loss: 0.6389 - acc: 0.7765 - val_loss: 1.0562 - val_acc: 0.6628

Epoch 00007: val_acc did not improve from 0.70270
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0126332412.
390/390 [==============================] - 56s 143ms/step - loss: 0.6003 - acc: 0.7893 - val_loss: 0.7048 - val_acc: 0.7646

Epoch 00008: val_acc improved from 0.70270 to 0.76460, saving model to weights_a3_5B.best.hdf5
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.011627907.
390/390 [==============================] - 56s 143ms/step - loss: 0.5604 - acc: 0.8039 - val_loss: 0.6630 - val_acc: 0.7721

Epoch 00009: val_acc improved from 0.76460 to 0.77210, saving model to weights_a3_5B.best.hdf5
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.0107707842.
390/390 [==============================] - 55s 142ms/step - loss: 0.5298 - acc: 0.8139 - val_loss: 0.6971 - val_acc: 0.7606

Epoch 00010: val_acc did not improve from 0.77210
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.010031348.
390/390 [==============================] - 56s 143ms/step - loss: 0.5024 - acc: 0.8260 - val_loss: 0.7219 - val_acc: 0.7597

Epoch 00011: val_acc did not improve from 0.77210
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.009386917.
390/390 [==============================] - 56s 143ms/step - loss: 0.4742 - acc: 0.8353 - val_loss: 0.5881 - val_acc: 0.7996

Epoch 00012: val_acc improved from 0.77210 to 0.79960, saving model to weights_a3_5B.best.hdf5
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.0088202867.
390/390 [==============================] - 56s 142ms/step - loss: 0.4482 - acc: 0.8434 - val_loss: 0.6737 - val_acc: 0.7844

Epoch 00013: val_acc did not improve from 0.79960
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.00831817.
390/390 [==============================] - 56s 142ms/step - loss: 0.4374 - acc: 0.8482 - val_loss: 0.6564 - val_acc: 0.7826

Epoch 00014: val_acc did not improve from 0.79960
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0078701426.
390/390 [==============================] - 56s 142ms/step - loss: 0.4158 - acc: 0.8540 - val_loss: 0.6355 - val_acc: 0.7886

Epoch 00015: val_acc did not improve from 0.79960
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0074679113.
390/390 [==============================] - 56s 143ms/step - loss: 0.3971 - acc: 0.8612 - val_loss: 0.5953 - val_acc: 0.8089

Epoch 00016: val_acc improved from 0.79960 to 0.80890, saving model to weights_a3_5B.best.hdf5
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.0071047957.
390/390 [==============================] - 55s 142ms/step - loss: 0.3798 - acc: 0.8644 - val_loss: 0.5967 - val_acc: 0.8051

Epoch 00017: val_acc did not improve from 0.80890
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0067753546.
390/390 [==============================] - 56s 143ms/step - loss: 0.3686 - acc: 0.8692 - val_loss: 0.5987 - val_acc: 0.8055

Epoch 00018: val_acc did not improve from 0.80890
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0064751113.
390/390 [==============================] - 56s 142ms/step - loss: 0.3549 - acc: 0.8750 - val_loss: 0.5544 - val_acc: 0.8187

Epoch 00019: val_acc improved from 0.80890 to 0.81870, saving model to weights_a3_5B.best.hdf5
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.0062003488.
390/390 [==============================] - 56s 143ms/step - loss: 0.3433 - acc: 0.8776 - val_loss: 0.5646 - val_acc: 0.8170

Epoch 00020: val_acc did not improve from 0.81870
Epoch 21/50

Epoch 00021: LearningRateScheduler setting learning rate to 0.0059479554.
  1/390 [..............................] - ETA: 57s - loss: 0.3023 - acc: 0.8984

/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.
  
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., initial_epoch=20, callbacks=[<keras.ca..., validation_data=(array([[[..., verbose=1, steps_per_epoch=390, epochs=50)`
  

390/390 [==============================] - 56s 143ms/step - loss: 0.3355 - acc: 0.8818 - val_loss: 0.6353 - val_acc: 0.7969

Epoch 00021: val_acc did not improve from 0.81870
Epoch 22/50

Epoch 00022: LearningRateScheduler setting learning rate to 0.0057153063.
390/390 [==============================] - 56s 144ms/step - loss: 0.3251 - acc: 0.8846 - val_loss: 0.5332 - val_acc: 0.8282

Epoch 00022: val_acc improved from 0.81870 to 0.82820, saving model to weights_a3_5B.best.hdf5
Epoch 23/50

Epoch 00023: LearningRateScheduler setting learning rate to 0.0055001719.
390/390 [==============================] - 56s 144ms/step - loss: 0.3193 - acc: 0.8878 - val_loss: 0.5699 - val_acc: 0.8216

Epoch 00023: val_acc did not improve from 0.82820
Epoch 24/50

Epoch 00024: LearningRateScheduler setting learning rate to 0.005300646.
390/390 [==============================] - 56s 144ms/step - loss: 0.3106 - acc: 0.8898 - val_loss: 0.5604 - val_acc: 0.8254

Epoch 00024: val_acc did not improve from 0.82820
Epoch 25/50

Epoch 00025: LearningRateScheduler setting learning rate to 0.0051150895.
390/390 [==============================] - 56s 143ms/step - loss: 0.2941 - acc: 0.8962 - val_loss: 0.5487 - val_acc: 0.8294

Epoch 00025: val_acc improved from 0.82820 to 0.82940, saving model to weights_a3_5B.best.hdf5
Epoch 26/50

Epoch 00026: LearningRateScheduler setting learning rate to 0.0049420849.
390/390 [==============================] - 56s 143ms/step - loss: 0.2953 - acc: 0.8953 - val_loss: 0.5772 - val_acc: 0.8232

Epoch 00026: val_acc did not improve from 0.82940
Epoch 27/50

Epoch 00027: LearningRateScheduler setting learning rate to 0.0047804004.
390/390 [==============================] - 56s 144ms/step - loss: 0.2850 - acc: 0.8980 - val_loss: 0.5617 - val_acc: 0.8276

Epoch 00027: val_acc did not improve from 0.82940
Epoch 28/50

Epoch 00028: LearningRateScheduler setting learning rate to 0.0046289599.
390/390 [==============================] - 56s 143ms/step - loss: 0.2721 - acc: 0.9030 - val_loss: 0.5595 - val_acc: 0.8297

Epoch 00028: val_acc improved from 0.82940 to 0.82970, saving model to weights_a3_5B.best.hdf5
Epoch 29/50

Epoch 00029: LearningRateScheduler setting learning rate to 0.00448682.
390/390 [==============================] - 56s 143ms/step - loss: 0.2703 - acc: 0.9022 - val_loss: 0.5962 - val_acc: 0.8205

Epoch 00029: val_acc did not improve from 0.82970
Epoch 30/50

Epoch 00030: LearningRateScheduler setting learning rate to 0.0043531492.
390/390 [==============================] - 56s 144ms/step - loss: 0.2633 - acc: 0.9049 - val_loss: 0.5870 - val_acc: 0.8227

Epoch 00030: val_acc did not improve from 0.82970
Epoch 31/50

Epoch 00031: LearningRateScheduler setting learning rate to 0.0042272127.
390/390 [==============================] - 56s 144ms/step - loss: 0.2559 - acc: 0.9083 - val_loss: 0.5390 - val_acc: 0.8363

Epoch 00031: val_acc improved from 0.82970 to 0.83630, saving model to weights_a3_5B.best.hdf5
Epoch 32/50

Epoch 00032: LearningRateScheduler setting learning rate to 0.0041083579.
390/390 [==============================] - 56s 144ms/step - loss: 0.2572 - acc: 0.9068 - val_loss: 0.5662 - val_acc: 0.8303

Epoch 00032: val_acc did not improve from 0.83630
Epoch 33/50

Epoch 00033: LearningRateScheduler setting learning rate to 0.003996004.
390/390 [==============================] - 56s 143ms/step - loss: 0.2425 - acc: 0.9128 - val_loss: 0.5647 - val_acc: 0.8363

Epoch 00033: val_acc did not improve from 0.83630
Epoch 34/50

Epoch 00034: LearningRateScheduler setting learning rate to 0.0038896317.
390/390 [==============================] - 56s 143ms/step - loss: 0.2400 - acc: 0.9123 - val_loss: 0.5550 - val_acc: 0.8368

Epoch 00034: val_acc improved from 0.83630 to 0.83680, saving model to weights_a3_5B.best.hdf5
Epoch 35/50

Epoch 00035: LearningRateScheduler setting learning rate to 0.0037887758.
390/390 [==============================] - 56s 144ms/step - loss: 0.2343 - acc: 0.9153 - val_loss: 0.5542 - val_acc: 0.8347

Epoch 00035: val_acc did not improve from 0.83680
Epoch 36/50

Epoch 00036: LearningRateScheduler setting learning rate to 0.0036930179.
390/390 [==============================] - 56s 143ms/step - loss: 0.2295 - acc: 0.9169 - val_loss: 0.5598 - val_acc: 0.8349

Epoch 00036: val_acc did not improve from 0.83680
Epoch 37/50

Epoch 00037: LearningRateScheduler setting learning rate to 0.0036019811.
390/390 [==============================] - 56s 143ms/step - loss: 0.2302 - acc: 0.9164 - val_loss: 0.5897 - val_acc: 0.8306

Epoch 00037: val_acc did not improve from 0.83680
Epoch 38/50

Epoch 00038: LearningRateScheduler setting learning rate to 0.0035153246.
390/390 [==============================] - 56s 144ms/step - loss: 0.2237 - acc: 0.9197 - val_loss: 0.5889 - val_acc: 0.8250

Epoch 00038: val_acc did not improve from 0.83680
Epoch 39/50

Epoch 00039: LearningRateScheduler setting learning rate to 0.0034327398.
390/390 [==============================] - 56s 144ms/step - loss: 0.2165 - acc: 0.9226 - val_loss: 0.5821 - val_acc: 0.8311

Epoch 00039: val_acc did not improve from 0.83680
Epoch 40/50

Epoch 00040: LearningRateScheduler setting learning rate to 0.0033539461.
390/390 [==============================] - 56s 144ms/step - loss: 0.2121 - acc: 0.9230 - val_loss: 0.5998 - val_acc: 0.8305

Epoch 00040: val_acc did not improve from 0.83680
Epoch 41/50

Epoch 00041: LearningRateScheduler setting learning rate to 0.0032786885.
390/390 [==============================] - 56s 144ms/step - loss: 0.2080 - acc: 0.9255 - val_loss: 0.5972 - val_acc: 0.8311

Epoch 00041: val_acc did not improve from 0.83680
Epoch 42/50

Epoch 00042: LearningRateScheduler setting learning rate to 0.0032067341.
390/390 [==============================] - 56s 144ms/step - loss: 0.2054 - acc: 0.9261 - val_loss: 0.5794 - val_acc: 0.8365

Epoch 00042: val_acc did not improve from 0.83680
Epoch 43/50

Epoch 00043: LearningRateScheduler setting learning rate to 0.0031378702.
390/390 [==============================] - 56s 144ms/step - loss: 0.2016 - acc: 0.9264 - val_loss: 0.6021 - val_acc: 0.8333

Epoch 00043: val_acc did not improve from 0.83680
Epoch 44/50

Epoch 00044: LearningRateScheduler setting learning rate to 0.0030719017.
390/390 [==============================] - 56s 144ms/step - loss: 0.1985 - acc: 0.9284 - val_loss: 0.6226 - val_acc: 0.8287

Epoch 00044: val_acc did not improve from 0.83680
Epoch 45/50

Epoch 00045: LearningRateScheduler setting learning rate to 0.0030086499.
390/390 [==============================] - 56s 144ms/step - loss: 0.2006 - acc: 0.9269 - val_loss: 0.5808 - val_acc: 0.8377

Epoch 00045: val_acc improved from 0.83680 to 0.83770, saving model to weights_a3_5B.best.hdf5
Epoch 46/50

Epoch 00046: LearningRateScheduler setting learning rate to 0.0029479503.
390/390 [==============================] - 56s 143ms/step - loss: 0.1900 - acc: 0.9305 - val_loss: 0.5941 - val_acc: 0.8363

Epoch 00046: val_acc did not improve from 0.83770
Epoch 47/50

Epoch 00047: LearningRateScheduler setting learning rate to 0.0028896514.
390/390 [==============================] - 56s 144ms/step - loss: 0.1937 - acc: 0.9296 - val_loss: 0.5738 - val_acc: 0.8407

Epoch 00047: val_acc improved from 0.83770 to 0.84070, saving model to weights_a3_5B.best.hdf5
Epoch 48/50

Epoch 00048: LearningRateScheduler setting learning rate to 0.0028336137.
390/390 [==============================] - 56s 144ms/step - loss: 0.1877 - acc: 0.9315 - val_loss: 0.5761 - val_acc: 0.8385

Epoch 00048: val_acc did not improve from 0.84070
Epoch 49/50

Epoch 00049: LearningRateScheduler setting learning rate to 0.0027797081.
390/390 [==============================] - 56s 144ms/step - loss: 0.1907 - acc: 0.9298 - val_loss: 0.5961 - val_acc: 0.8364

Epoch 00049: val_acc did not improve from 0.84070
Epoch 50/50

Epoch 00050: LearningRateScheduler setting learning rate to 0.0027278152.
390/390 [==============================] - 56s 144ms/step - loss: 0.1842 - acc: 0.9321 - val_loss: 0.5972 - val_acc: 0.8353

Epoch 00050: val_acc did not improve from 0.84070
Model took 1681.35 seconds to train


## Model Defintion

      # Define the model
      ## Output : (None, 32, 32, 96) 
      ## RF 7
      # Define the model
      model = Sequential()
      model.add(SeparableConv2D(48, 3, 3, border_mode='same', input_shape=(32, 32, 3)))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## OP: (None, 32, 32, 48)
      ## RF: 3

      model.add(SeparableConv2D(48, 3, 3))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output : (None, 30, 30, 48) 
      ## RF: 5

      # model.add(MaxPooling2D(pool_size=(2, 2)))
      # model.add(SeparableConv2D(48, 3, 3),dilation_rate=(3, 3)))
      model.add(SeparableConv2D(filters=48, kernel_size=(3, 3), dilation_rate=(2, 2),border_mode='valid'))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output: (None, 26, 26, 48)
      ## RF: 9

      model.add(Dropout(0.25))

      model.add(SeparableConv2D(96, 3, 3, border_mode='same'))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output: (None, 26, 26, 96)
      ## RF: 11

      model.add(SeparableConv2D(96, 3, 3))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output (None, 24, 24, 96)
      ## RF: 13

      model.add(MaxPooling2D(pool_size=(2, 2)))
      model.add(BatchNormalization())
      model.add(Dropout(0.25))
      ## Output (None, 12, 12, 96)
      ## RF: 15

      model.add(SeparableConv2D(192, 3, 3, border_mode='same'))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output : (None, 12, 12, 192)
      ## RF: 17

      model.add(SeparableConv2D(192, 3, 3))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output (None, 10, 10, 192)
      ## RF: 19

      model.add(MaxPooling2D(pool_size=(2, 2)))
      ## output (None, 5, 5, 192)
      ## RF: 38

      model.add(Dropout(0.25))

      model.add(SeparableConv2D(filters=10, kernel_size=(3, 3), dilation_rate=(2, 2),border_mode='same'))
      model.add(BatchNormalization())
      ## output (None, 5, 5, 10)
      ## RF: 42

      model.add(GlobalAveragePooling2D())

      model.add(Dense(num_classes, activation='softmax'))
      # Compile the model
      model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
