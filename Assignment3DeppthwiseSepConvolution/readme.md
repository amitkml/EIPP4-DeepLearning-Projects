## Baseline Model log

Epoch 1/50
390/390 [==============================] - 19s 48ms/step - loss: 1.8634 - acc: 0.2853 - val_loss: 1.5148 - val_acc: 0.4386
Epoch 2/50
390/390 [==============================] - 8s 20ms/step - loss: 1.4027 - acc: 0.4860 - val_loss: 1.1998 - val_acc: 0.5703
Epoch 3/50
390/390 [==============================] - 8s 20ms/step - loss: 1.1777 - acc: 0.5821 - val_loss: 1.0308 - val_acc: 0.6368
Epoch 4/50
390/390 [==============================] - 8s 20ms/step - loss: 1.0224 - acc: 0.6439 - val_loss: 0.9538 - val_acc: 0.6636
Epoch 5/50
390/390 [==============================] - 8s 20ms/step - loss: 0.9232 - acc: 0.6780 - val_loss: 0.8299 - val_acc: 0.7131
Epoch 6/50
390/390 [==============================] - 8s 20ms/step - loss: 0.8387 - acc: 0.7097 - val_loss: 0.7559 - val_acc: 0.7405
Epoch 7/50
390/390 [==============================] - 8s 20ms/step - loss: 0.7815 - acc: 0.7312 - val_loss: 0.7509 - val_acc: 0.7439
Epoch 8/50
390/390 [==============================] - 8s 20ms/step - loss: 0.7299 - acc: 0.7499 - val_loss: 0.7800 - val_acc: 0.7348
Epoch 9/50
390/390 [==============================] - 8s 20ms/step - loss: 0.6847 - acc: 0.7657 - val_loss: 0.6620 - val_acc: 0.7753
Epoch 10/50
390/390 [==============================] - 8s 20ms/step - loss: 0.6507 - acc: 0.7784 - val_loss: 0.6480 - val_acc: 0.7784
Epoch 11/50
390/390 [==============================] - 8s 20ms/step - loss: 0.6238 - acc: 0.7868 - val_loss: 0.6286 - val_acc: 0.7855
Epoch 12/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5974 - acc: 0.7971 - val_loss: 0.6494 - val_acc: 0.7803
Epoch 13/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5776 - acc: 0.8040 - val_loss: 0.6414 - val_acc: 0.7827
Epoch 14/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5521 - acc: 0.8128 - val_loss: 0.6094 - val_acc: 0.7939
Epoch 15/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5355 - acc: 0.8184 - val_loss: 0.6295 - val_acc: 0.7898
Epoch 16/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5220 - acc: 0.8227 - val_loss: 0.5882 - val_acc: 0.8042
Epoch 17/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5126 - acc: 0.8254 - val_loss: 0.6110 - val_acc: 0.8006
Epoch 18/50
390/390 [==============================] - 8s 20ms/step - loss: 0.5016 - acc: 0.8281 - val_loss: 0.5853 - val_acc: 0.8075
Epoch 19/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4856 - acc: 0.8341 - val_loss: 0.5924 - val_acc: 0.8031
Epoch 20/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4783 - acc: 0.8362 - val_loss: 0.6104 - val_acc: 0.7990
Epoch 21/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4623 - acc: 0.8409 - val_loss: 0.5820 - val_acc: 0.8179
Epoch 22/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4410 - acc: 0.8501 - val_loss: 0.6177 - val_acc: 0.7998
Epoch 23/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4361 - acc: 0.8518 - val_loss: 0.6214 - val_acc: 0.8003
Epoch 24/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4356 - acc: 0.8501 - val_loss: 0.5838 - val_acc: 0.8172
Epoch 25/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4196 - acc: 0.8556 - val_loss: 0.5560 - val_acc: 0.8205
Epoch 26/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4189 - acc: 0.8593 - val_loss: 0.5949 - val_acc: 0.8096
Epoch 27/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4156 - acc: 0.8577 - val_loss: 0.5723 - val_acc: 0.8159
Epoch 28/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4066 - acc: 0.8609 - val_loss: 0.5921 - val_acc: 0.8166
Epoch 29/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3957 - acc: 0.8656 - val_loss: 0.6035 - val_acc: 0.8090
Epoch 30/50
390/390 [==============================] - 8s 20ms/step - loss: 0.4018 - acc: 0.8642 - val_loss: 0.5739 - val_acc: 0.8190
Epoch 31/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3839 - acc: 0.8695 - val_loss: 0.5889 - val_acc: 0.8191
Epoch 32/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3805 - acc: 0.8712 - val_loss: 0.5799 - val_acc: 0.8186
Epoch 33/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3822 - acc: 0.8702 - val_loss: 0.6240 - val_acc: 0.8064
Epoch 34/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3721 - acc: 0.8746 - val_loss: 0.5895 - val_acc: 0.8147
Epoch 35/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3662 - acc: 0.8755 - val_loss: 0.5635 - val_acc: 0.8210
Epoch 36/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3686 - acc: 0.8746 - val_loss: 0.5714 - val_acc: 0.8220
Epoch 37/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3608 - acc: 0.8780 - val_loss: 0.5979 - val_acc: 0.8131
Epoch 38/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3636 - acc: 0.8788 - val_loss: 0.5871 - val_acc: 0.8206
Epoch 39/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3508 - acc: 0.8825 - val_loss: 0.5780 - val_acc: 0.8211
Epoch 40/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3564 - acc: 0.8813 - val_loss: 0.5899 - val_acc: 0.8223
Epoch 41/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3425 - acc: 0.8834 - val_loss: 0.5839 - val_acc: 0.8201
Epoch 42/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3473 - acc: 0.8834 - val_loss: 0.6109 - val_acc: 0.8123
Epoch 43/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3377 - acc: 0.8845 - val_loss: 0.5778 - val_acc: 0.8230
Epoch 44/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3347 - acc: 0.8877 - val_loss: 0.5740 - val_acc: 0.8203
Epoch 45/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3378 - acc: 0.8870 - val_loss: 0.5794 - val_acc: 0.8287
Epoch 46/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3301 - acc: 0.8886 - val_loss: 0.6280 - val_acc: 0.8116
Epoch 47/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3321 - acc: 0.8891 - val_loss: 0.6077 - val_acc: 0.8185
Epoch 48/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3244 - acc: 0.8927 - val_loss: 0.6317 - val_acc: 0.8179
Epoch 49/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3293 - acc: 0.8896 - val_loss: 0.5761 - val_acc: 0.8254
Epoch 50/50
390/390 [==============================] - 8s 20ms/step - loss: 0.3232 - acc: 0.8919 - val_loss: 0.5906 - val_acc: 0.8245
Model took 397.36 seconds to train

## Depthwise Separable Convolution Log

Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.0032.
390/390 [==============================] - 111s 285ms/step - loss: 1.6049 - acc: 0.4108 - val_loss: 4.7465 - val_acc: 0.3747

Epoch 00001: val_acc improved from -inf to 0.37470, saving model to weights_a3_4.best.hdf5
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0026251025.
390/390 [==============================] - 102s 262ms/step - loss: 1.2234 - acc: 0.5629 - val_loss: 2.0710 - val_acc: 0.4759

Epoch 00002: val_acc improved from 0.37470 to 0.47590, saving model to weights_a3_4.best.hdf5
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0022253129.
390/390 [==============================] - 102s 262ms/step - loss: 1.0382 - acc: 0.6325 - val_loss: 1.1438 - val_acc: 0.6210

Epoch 00003: val_acc improved from 0.47590 to 0.62100, saving model to weights_a3_4.best.hdf5
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.001931201.
390/390 [==============================] - 102s 262ms/step - loss: 0.9122 - acc: 0.6775 - val_loss: 1.0304 - val_acc: 0.6533

Epoch 00004: val_acc improved from 0.62100 to 0.65330, saving model to weights_a3_4.best.hdf5
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0017057569.
390/390 [==============================] - 102s 261ms/step - loss: 0.8152 - acc: 0.7136 - val_loss: 0.9086 - val_acc: 0.6916

Epoch 00005: val_acc improved from 0.65330 to 0.69160, saving model to weights_a3_4.best.hdf5
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0015274463.
390/390 [==============================] - 103s 263ms/step - loss: 0.7279 - acc: 0.7450 - val_loss: 0.9011 - val_acc: 0.7047

Epoch 00006: val_acc improved from 0.69160 to 0.70470, saving model to weights_a3_4.best.hdf5
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.0013828868.
390/390 [==============================] - 102s 263ms/step - loss: 0.6590 - acc: 0.7700 - val_loss: 0.7598 - val_acc: 0.7450

Epoch 00007: val_acc improved from 0.70470 to 0.74500, saving model to weights_a3_4.best.hdf5
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0012633241.
390/390 [==============================] - 103s 263ms/step - loss: 0.5934 - acc: 0.7933 - val_loss: 0.8893 - val_acc: 0.7187

Epoch 00008: val_acc did not improve from 0.74500
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.0011627907.
390/390 [==============================] - 102s 262ms/step - loss: 0.5351 - acc: 0.8132 - val_loss: 0.7769 - val_acc: 0.7512

Epoch 00009: val_acc improved from 0.74500 to 0.75120, saving model to weights_a3_4.best.hdf5
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.0010770784.
390/390 [==============================] - 102s 263ms/step - loss: 0.4832 - acc: 0.8302 - val_loss: 0.7408 - val_acc: 0.7624

Epoch 00010: val_acc improved from 0.75120 to 0.76240, saving model to weights_a3_4.best.hdf5
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.0010031348.
390/390 [==============================] - 102s 262ms/step - loss: 0.4312 - acc: 0.8484 - val_loss: 0.7396 - val_acc: 0.7659

Epoch 00011: val_acc improved from 0.76240 to 0.76590, saving model to weights_a3_4.best.hdf5
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.0009386917.
390/390 [==============================] - 102s 262ms/step - loss: 0.3882 - acc: 0.8638 - val_loss: 0.8206 - val_acc: 0.7523

Epoch 00012: val_acc did not improve from 0.76590
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.0008820287.
390/390 [==============================] - 102s 262ms/step - loss: 0.3428 - acc: 0.8815 - val_loss: 0.7346 - val_acc: 0.7806

Epoch 00013: val_acc improved from 0.76590 to 0.78060, saving model to weights_a3_4.best.hdf5
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.000831817.
390/390 [==============================] - 102s 263ms/step - loss: 0.3025 - acc: 0.8945 - val_loss: 0.7836 - val_acc: 0.7745

Epoch 00014: val_acc did not improve from 0.78060
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0007870143.
390/390 [==============================] - 103s 263ms/step - loss: 0.2726 - acc: 0.9044 - val_loss: 0.7406 - val_acc: 0.7842

Epoch 00015: val_acc improved from 0.78060 to 0.78420, saving model to weights_a3_4.best.hdf5
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0007467911.
390/390 [==============================] - 102s 263ms/step - loss: 0.2420 - acc: 0.9161 - val_loss: 0.7832 - val_acc: 0.7851

Epoch 00016: val_acc improved from 0.78420 to 0.78510, saving model to weights_a3_4.best.hdf5
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.0007104796.
390/390 [==============================] - 102s 263ms/step - loss: 0.2150 - acc: 0.9255 - val_loss: 0.8009 - val_acc: 0.7873

Epoch 00017: val_acc improved from 0.78510 to 0.78730, saving model to weights_a3_4.best.hdf5
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0006775355.
390/390 [==============================] - 103s 263ms/step - loss: 0.1985 - acc: 0.9306 - val_loss: 0.8029 - val_acc: 0.7818

Epoch 00018: val_acc did not improve from 0.78730
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0006475111.
390/390 [==============================] - 103s 264ms/step - loss: 0.1730 - acc: 0.9405 - val_loss: 0.8708 - val_acc: 0.7780

Epoch 00019: val_acc did not improve from 0.78730
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.0006200349.
390/390 [==============================] - 103s 263ms/step - loss: 0.1641 - acc: 0.9430 - val_loss: 0.8317 - val_acc: 0.7884

Epoch 00020: val_acc improved from 0.78730 to 0.78840, saving model to weights_a3_4.best.hdf5
Model took 2063.85 seconds to train
Epoch 00021: LearningRateScheduler setting learning rate to 0.0005947955.
390/390 [==============================] - 102s 262ms/step - loss: 0.4051 - acc: 0.8633 - val_loss: 0.7637 - val_acc: 0.7936

Epoch 00021: val_acc improved from 0.78840 to 0.79360, saving model to weights_a3_4.best.hdf5
Epoch 22/50

Epoch 00022: LearningRateScheduler setting learning rate to 0.0005715306.
390/390 [==============================] - 102s 262ms/step - loss: 0.3522 - acc: 0.8777 - val_loss: 0.7676 - val_acc: 0.7882

Epoch 00022: val_acc did not improve from 0.79360
Epoch 23/50

Epoch 00023: LearningRateScheduler setting learning rate to 0.0005500172.
390/390 [==============================] - 102s 262ms/step - loss: 0.3266 - acc: 0.8861 - val_loss: 0.7874 - val_acc: 0.7878

Epoch 00023: val_acc did not improve from 0.79360
Epoch 24/50

Epoch 00024: LearningRateScheduler setting learning rate to 0.0005300646.
390/390 [==============================] - 102s 262ms/step - loss: 0.3110 - acc: 0.8923 - val_loss: 0.6724 - val_acc: 0.8075

Epoch 00024: val_acc improved from 0.79360 to 0.80750, saving model to weights_a3_4.best.hdf5
Epoch 25/50

Epoch 00025: LearningRateScheduler setting learning rate to 0.000511509.
390/390 [==============================] - 102s 262ms/step - loss: 0.2938 - acc: 0.8974 - val_loss: 0.7151 - val_acc: 0.7952

Epoch 00025: val_acc did not improve from 0.80750
Epoch 26/50

Epoch 00026: LearningRateScheduler setting learning rate to 0.0004942085.
390/390 [==============================] - 102s 262ms/step - loss: 0.2814 - acc: 0.9035 - val_loss: 0.6806 - val_acc: 0.8072

Epoch 00026: val_acc did not improve from 0.80750
Epoch 27/50

Epoch 00027: LearningRateScheduler setting learning rate to 0.00047804.
390/390 [==============================] - 102s 262ms/step - loss: 0.2700 - acc: 0.9064 - val_loss: 0.7183 - val_acc: 0.7991

Epoch 00027: val_acc did not improve from 0.80750
Epoch 28/50

Epoch 00028: LearningRateScheduler setting learning rate to 0.000462896.
390/390 [==============================] - 102s 262ms/step - loss: 0.2601 - acc: 0.9086 - val_loss: 0.7371 - val_acc: 0.8003

Epoch 00028: val_acc did not improve from 0.80750
Epoch 29/50

Epoch 00029: LearningRateScheduler setting learning rate to 0.000448682.
390/390 [==============================] - 102s 262ms/step - loss: 0.2490 - acc: 0.9143 - val_loss: 0.6983 - val_acc: 0.8068

Epoch 00029: val_acc did not improve from 0.80750
Epoch 30/50

Epoch 00030: LearningRateScheduler setting learning rate to 0.0004353149.
390/390 [==============================] - 102s 262ms/step - loss: 0.2305 - acc: 0.9195 - val_loss: 0.6857 - val_acc: 0.8128

Epoch 00030: val_acc improved from 0.80750 to 0.81280, saving model to weights_a3_4.best.hdf5
Epoch 31/50

Epoch 00031: LearningRateScheduler setting learning rate to 0.0004227213.
390/390 [==============================] - 103s 263ms/step - loss: 0.2262 - acc: 0.9221 - val_loss: 0.7222 - val_acc: 0.8054

Epoch 00031: val_acc did not improve from 0.81280
Epoch 32/50

Epoch 00032: LearningRateScheduler setting learning rate to 0.0004108358.
390/390 [==============================] - 102s 262ms/step - loss: 0.2215 - acc: 0.9228 - val_loss: 0.7071 - val_acc: 0.8088

Epoch 00032: val_acc did not improve from 0.81280
Epoch 33/50

Epoch 00033: LearningRateScheduler setting learning rate to 0.0003996004.
390/390 [==============================] - 102s 262ms/step - loss: 0.2150 - acc: 0.9257 - val_loss: 0.7215 - val_acc: 0.8096

Epoch 00033: val_acc did not improve from 0.81280
Epoch 34/50

Epoch 00034: LearningRateScheduler setting learning rate to 0.0003889632.
390/390 [==============================] - 102s 262ms/step - loss: 0.2039 - acc: 0.9289 - val_loss: 0.7857 - val_acc: 0.8020

Epoch 00034: val_acc did not improve from 0.81280
Epoch 35/50

Epoch 00035: LearningRateScheduler setting learning rate to 0.0003788776.
390/390 [==============================] - 103s 263ms/step - loss: 0.1966 - acc: 0.9335 - val_loss: 0.8018 - val_acc: 0.7965

Epoch 00035: val_acc did not improve from 0.81280
Epoch 36/50

Epoch 00036: LearningRateScheduler setting learning rate to 0.0003693018.
390/390 [==============================] - 103s 263ms/step - loss: 0.1911 - acc: 0.9340 - val_loss: 0.7107 - val_acc: 0.8128

Epoch 00036: val_acc did not improve from 0.81280
Epoch 37/50

Epoch 00037: LearningRateScheduler setting learning rate to 0.0003601981.
390/390 [==============================] - 102s 262ms/step - loss: 0.1832 - acc: 0.9361 - val_loss: 0.7295 - val_acc: 0.8097

Epoch 00037: val_acc did not improve from 0.81280
Epoch 38/50

Epoch 00038: LearningRateScheduler setting learning rate to 0.0003515325.
390/390 [==============================] - 103s 264ms/step - loss: 0.1826 - acc: 0.9370 - val_loss: 0.7495 - val_acc: 0.8115

Epoch 00038: val_acc did not improve from 0.81280
Epoch 39/50

Epoch 00039: LearningRateScheduler setting learning rate to 0.000343274.
390/390 [==============================] - 103s 263ms/step - loss: 0.1667 - acc: 0.9434 - val_loss: 0.7764 - val_acc: 0.8072

Epoch 00039: val_acc did not improve from 0.81280
Epoch 40/50

Epoch 00040: LearningRateScheduler setting learning rate to 0.0003353946.
390/390 [==============================] - 103s 264ms/step - loss: 0.1712 - acc: 0.9408 - val_loss: 0.7743 - val_acc: 0.8056

Epoch 00040: val_acc did not improve from 0.81280
Epoch 41/50

Epoch 00041: LearningRateScheduler setting learning rate to 0.0003278689.
390/390 [==============================] - 103s 264ms/step - loss: 0.1595 - acc: 0.9446 - val_loss: 0.7489 - val_acc: 0.8174

Epoch 00041: val_acc improved from 0.81280 to 0.81740, saving model to weights_a3_4.best.hdf5
Epoch 42/50

Epoch 00042: LearningRateScheduler setting learning rate to 0.0003206734.
390/390 [==============================] - 103s 264ms/step - loss: 0.1596 - acc: 0.9454 - val_loss: 0.7595 - val_acc: 0.8106

Epoch 00042: val_acc did not improve from 0.81740
Epoch 43/50

Epoch 00043: LearningRateScheduler setting learning rate to 0.000313787.
390/390 [==============================] - 103s 264ms/step - loss: 0.1539 - acc: 0.9476 - val_loss: 0.7641 - val_acc: 0.8115

Epoch 00043: val_acc did not improve from 0.81740
Epoch 44/50

Epoch 00044: LearningRateScheduler setting learning rate to 0.0003071902.
390/390 [==============================] - 103s 264ms/step - loss: 0.1521 - acc: 0.9480 - val_loss: 0.7467 - val_acc: 0.8163

Epoch 00044: val_acc did not improve from 0.81740
Epoch 45/50

Epoch 00045: LearningRateScheduler setting learning rate to 0.000300865.
390/390 [==============================] - 103s 264ms/step - loss: 0.1455 - acc: 0.9503 - val_loss: 0.8146 - val_acc: 0.8068

Epoch 00045: val_acc did not improve from 0.81740
Epoch 46/50

Epoch 00046: LearningRateScheduler setting learning rate to 0.000294795.
390/390 [==============================] - 103s 264ms/step - loss: 0.1419 - acc: 0.9510 - val_loss: 0.7866 - val_acc: 0.8097

Epoch 00046: val_acc did not improve from 0.81740
Epoch 47/50

Epoch 00047: LearningRateScheduler setting learning rate to 0.0002889651.
390/390 [==============================] - 103s 264ms/step - loss: 0.1415 - acc: 0.9516 - val_loss: 0.7951 - val_acc: 0.8073

Epoch 00047: val_acc did not improve from 0.81740
Epoch 48/50

Epoch 00048: LearningRateScheduler setting learning rate to 0.0002833614.
390/390 [==============================] - 103s 264ms/step - loss: 0.1379 - acc: 0.9535 - val_loss: 0.7561 - val_acc: 0.8201

Epoch 00048: val_acc improved from 0.81740 to 0.82010, saving model to weights_a3_4.best.hdf5
Epoch 49/50

Epoch 00049: LearningRateScheduler setting learning rate to 0.0002779708.
390/390 [==============================] - 103s 264ms/step - loss: 0.1302 - acc: 0.9554 - val_loss: 0.7628 - val_acc: 0.8175

Epoch 00049: val_acc did not improve from 0.82010
Epoch 50/50

Epoch 00050: LearningRateScheduler setting learning rate to 0.0002727815.
390/390 [==============================] - 103s 263ms/step - loss: 0.1297 - acc: 0.9555 - val_loss: 0.7644 - val_acc: 0.8180

Epoch 00051: LearningRateScheduler setting learning rate to 0.0002677824.
390/390 [==============================] - 86s 220ms/step - loss: 0.6858 - acc: 0.7710 - val_loss: 0.5582 - val_acc: 0.8187

Epoch 00051: val_acc did not improve from 0.82800
Epoch 52/60

Epoch 00052: LearningRateScheduler setting learning rate to 0.0002629633.
390/390 [==============================] - 86s 219ms/step - loss: 0.6413 - acc: 0.7813 - val_loss: 0.6129 - val_acc: 0.8051

Epoch 00052: val_acc did not improve from 0.82800
Epoch 53/60

Epoch 00053: LearningRateScheduler setting learning rate to 0.0002583145.
390/390 [==============================] - 86s 220ms/step - loss: 0.6261 - acc: 0.7869 - val_loss: 0.5152 - val_acc: 0.8312

Epoch 00053: val_acc improved from 0.82800 to 0.83120, saving model to weights_a3_4.best.hdf5
Epoch 54/60

Epoch 00054: LearningRateScheduler setting learning rate to 0.0002538272.
390/390 [==============================] - 86s 219ms/step - loss: 0.6055 - acc: 0.7959 - val_loss: 0.5549 - val_acc: 0.8197

Epoch 00054: val_acc did not improve from 0.83120
Epoch 55/60

Epoch 00055: LearningRateScheduler setting learning rate to 0.0002494932.
390/390 [==============================] - 86s 220ms/step - loss: 0.5946 - acc: 0.7980 - val_loss: 0.5747 - val_acc: 0.8150

Epoch 00055: val_acc did not improve from 0.83120
Epoch 56/60

Epoch 00056: LearningRateScheduler setting learning rate to 0.0002453047.
390/390 [==============================] - 86s 219ms/step - loss: 0.5836 - acc: 0.8026 - val_loss: 0.5447 - val_acc: 0.8225

Epoch 00056: val_acc did not improve from 0.83120
Epoch 57/60

Epoch 00057: LearningRateScheduler setting learning rate to 0.0002412545.
390/390 [==============================] - 86s 220ms/step - loss: 0.5704 - acc: 0.8065 - val_loss: 0.5427 - val_acc: 0.8213

Epoch 00057: val_acc did not improve from 0.83120
Epoch 58/60

Epoch 00058: LearningRateScheduler setting learning rate to 0.0002373359.
390/390 [==============================] - 86s 222ms/step - loss: 0.5701 - acc: 0.8060 - val_loss: 0.5433 - val_acc: 0.8240

Epoch 00058: val_acc did not improve from 0.83120
Epoch 59/60

Epoch 00059: LearningRateScheduler setting learning rate to 0.0002335425.
390/390 [==============================] - 86s 222ms/step - loss: 0.5557 - acc: 0.8126 - val_loss: 0.5382 - val_acc: 0.8269

Epoch 00059: val_acc did not improve from 0.83120
Epoch 60/60

Epoch 00060: LearningRateScheduler setting learning rate to 0.0002298685.
390/390 [==============================] - 86s 220ms/step - loss: 0.5525 - acc: 0.8119 - val_loss: 0.5136 - val_acc: 0.8279

Epoch 00060: val_acc did not improve from 0.83120

## Model Defintion

      # Define the model
      model = Sequential()
      model.add(SeparableConv2D(filters=24*2*2, kernel_size=(3, 3), dilation_rate=(3, 3), input_shape=(32, 32, 3),border_mode='same'))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output : (None, 32, 32, 96) 
      ## RF 7

      model.add(SeparableConv2D(filters=48*2*2, kernel_size=(3, 3),dilation_rate=(3, 3)))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output : (None, 26, 26, 192) 
      ## RF 13

      # model.add(MaxPooling2D(pool_size=(2, 2)))
      # model.add(Dropout(0.25))

      model.add(SeparableConv2D(filters=96*2*2, kernel_size=(3, 3), dilation_rate=(2, 2),border_mode='valid'))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output : (None, 22, 22, 384) 
      ## RF 19


      model.add(SeparableConv2D(filters=192*2*2, kernel_size=(3, 3),dilation_rate=(2, 2)))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output :  (None, 18, 18, 768)
      ## RF 23


      # model.add(MaxPooling2D(pool_size=(2, 2)))
      model.add(Dropout(0.25))

      model.add(SeparableConv2D(filters=384*2*2, kernel_size=(3, 3), dilation_rate=(2, 2),border_mode='valid'))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output :  (None, 14, 14, 768)
      ## RF 23

      model.add(MaxPooling2D(pool_size=(2, 2)))
      ## Output :  (None, 7, 7, 768)
      ## RF 46


      model.add(SeparableConv2D(filters=48*2*2, kernel_size=(3, 3), dilation_rate=(3, 3),border_mode='same'))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      model.add(Dropout(0.25))
      ## Output :  (None, 7, 7, 768)
      ## RF 54

      model.add(SeparableConv2D(filters=96*2*2, kernel_size=(3, 3), dilation_rate=(3, 3),border_mode='same'))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output :  (None, 7, 7, 384)
      ## RF 60

      model.add(SeparableConv2D(filters=192*2*2, kernel_size=(3, 3), dilation_rate=(3, 3),border_mode='same'))
      model.add(BatchNormalization())
      model.add(Activation('relu'))
      ## Output :  (None, 7, 7, 768)
      ## RF 66

      model.add(SeparableConv2D(filters=384*2*2, kernel_size=(3, 3), dilation_rate=(3, 3),border_mode='same'))
      model.add(BatchNormalization())
      ## Output :  (None, 7, 7, 1692)
      ## RF 72

      # model.add(MaxPooling2D(pool_size=(2, 2)))
      model.add(Dropout(0.35))

      model.add(SeparableConv2D(filters=10, kernel_size=(3, 3), dilation_rate=(2, 2),border_mode='same'))
      model.add(BatchNormalization())

      # model.add(SeparableConv2D(filters=96*2, kernel_size=(1, 1), dilation_rate=(2, 2),border_mode='same'))
      # model.add(BatchNormalization())

      # model.add(Flatten())

      model.add(GlobalAveragePooling2D())

      # model.add(Dense(512))
      # model.add(Activation('relu'))
      # model.add(Dropout(0.35))

      # model.add(Dense(128))
      # model.add(Activation('relu'))
      # model.add(Dropout(0.2))

      model.add(Dense(num_classes, activation='softmax'))
      # Compile the model
      model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])
      # Compile the model
      # model.compile(optimizer=Adam(lr=0.001,decay=0, beta_1=0.9, beta_2=0.999, epsilon=1e-08),loss='categorical_crossentropy', metrics=['accuracy'])
